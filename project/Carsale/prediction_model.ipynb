{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5100ff3d-3aea-46c2-8380-71c08f2209a4",
   "metadata": {},
   "source": [
    "# Car Price Prediction Model Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc27040-ad01-4788-ba76-5ea0d1799298",
   "metadata": {},
   "source": [
    "Prepared by *Lau Wen Jun*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec46684",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Background](##1-background)\n",
    "2. [Methodology](##2-methodology)\n",
    "3. [Variables](##3-variables)\n",
    "4. [Analysis](##4-analysis)\n",
    "    - 4.1. [Import Libraries](###41-import-libraries)\n",
    "    - 4.2. [Data Preprocessing](###42-data-preprocessing)\n",
    "    - 4.3. [Feature Engineering](###43-feature-engineering)\n",
    "    - 4.4. [Feature Importance Analysis](###44-feature-importance-analysis)\n",
    "    - 4.5. [Model Development & Performance Validation](###45-model-development--performance-validation)\n",
    "    - 4.6. [Result & Export](###46-result--export) \n",
    "5. [Discussion and Conclusion](##5-discussion-and-conclusion)\n",
    "    - 5.1. [Discussion](##51-discussion)\n",
    "    - 5.2. [Conclusion](##52-conclusion)\n",
    "6. [Appendix: Function](##6-appendix-function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7eb8ee5-4afd-4bb6-86df-5abc286a089f",
   "metadata": {},
   "source": [
    "## 1. Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c9daa6-ac2b-438b-9545-5f93868795cb",
   "metadata": {},
   "source": [
    "This analysis focuses on developing a machine learning model to predict second-hand car prices using a dataset containing various car attributes and historical sales information. The dataset includes hashed identifiers for car brands and models, along with specific vehicle characteristics, condition indicators, and temporal data. The goal is to create an accurate price prediction model that can assist in valuing used cars based on their features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2142ee8-056b-484f-a65d-acbcbe3ed658",
   "metadata": {},
   "source": [
    "## 2. Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64071979-c98b-4a38-ac85-08befa890467",
   "metadata": {},
   "source": [
    "The analysis methodology comprises three main phases: data preprocessing, feature engineering, and model development. The preprocessing phase focuses on data quality and standardization, including handling missing values, converting date fields to appropriate formats, and encoding categorical variables using label encoding to make them suitable for machine learning algorithms.\n",
    "\n",
    "Feature engineering enhances the dataset's predictive power by creating meaningful new features. This includes deriving temporal features from dates (such as days_until_sale, sales_month, and car_age), developing interaction features between related variables (age_milage combining car age and mileage), and constructing aggregate indicators like damage_score to capture the combined effect of accident and flood damage.\n",
    "\n",
    "The model development phase begins with feature selection based on importance thresholds to identify the most influential predictors. Three different machine learning models are implemented and compared: Random Forest, which creates multiple decision trees and combines their predictions through voting, making it robust against outliers and good at capturing complex patterns; XGBoost, which builds trees sequentially where each new tree focuses on correcting the mistakes of previous trees, making it particularly effective for structured data like our car dataset; and Gradient Boosting, which works similarly to XGBoost but with a different optimization approach for building trees. These models are evaluated using complementary metrics: Root Mean Square Error (RMSE) to penalize large prediction errors, Mean Absolute Error (MAE) for interpretable error measurements, and R-squared (R²) to assess the model's overall explanatory power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7cfaaa-b64d-4379-b112-90b4479aea2c",
   "metadata": {},
   "source": [
    "## 3. Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3941977-4eee-4950-a19e-95152e95a48f",
   "metadata": {},
   "source": [
    "The \"Car Sales Data\" contains hashed data regarding second hand car prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "add21f92-24b7-4fef-88da-7cd3c15db24e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_d4ac1 th {\n",
       "  text-align: center;\n",
       "  background-color: #f2f2f2;\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_d4ac1_row0_col0, #T_d4ac1_row0_col1, #T_d4ac1_row1_col0, #T_d4ac1_row1_col1, #T_d4ac1_row2_col0, #T_d4ac1_row2_col1, #T_d4ac1_row3_col0, #T_d4ac1_row3_col1, #T_d4ac1_row4_col0, #T_d4ac1_row4_col1, #T_d4ac1_row5_col0, #T_d4ac1_row5_col1, #T_d4ac1_row6_col0, #T_d4ac1_row6_col1, #T_d4ac1_row7_col0, #T_d4ac1_row7_col1, #T_d4ac1_row8_col0, #T_d4ac1_row8_col1, #T_d4ac1_row9_col0, #T_d4ac1_row9_col1, #T_d4ac1_row10_col0, #T_d4ac1_row10_col1, #T_d4ac1_row11_col0, #T_d4ac1_row11_col1, #T_d4ac1_row12_col0, #T_d4ac1_row12_col1 {\n",
       "  text-align: left;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_d4ac1\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_d4ac1_level0_col0\" class=\"col_heading level0 col0\" >Variable</th>\n",
       "      <th id=\"T_d4ac1_level0_col1\" class=\"col_heading level0 col1\" >Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_d4ac1_row0_col0\" class=\"data row0 col0\" >car_brand</td>\n",
       "      <td id=\"T_d4ac1_row0_col1\" class=\"data row0 col1\" >Hashed value for a car brand (Honda, BMW, Proton, etc.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d4ac1_row1_col0\" class=\"data row1 col0\" >car_model</td>\n",
       "      <td id=\"T_d4ac1_row1_col1\" class=\"data row1 col1\" >Hashed value for a car model (Myvi, Saga, i3, etc.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d4ac1_row2_col0\" class=\"data row2 col0\" >car_variant</td>\n",
       "      <td id=\"T_d4ac1_row2_col1\" class=\"data row2 col1\" >Hashed value for a car variant. Car variants can be repeated with different car brands and models. For example, Perodua Myvi & Honda City can both have variant G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d4ac1_row3_col0\" class=\"data row3 col0\" >car_year</td>\n",
       "      <td id=\"T_d4ac1_row3_col1\" class=\"data row3 col1\" >Car production year as 4 digits integer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d4ac1_row4_col0\" class=\"data row4 col0\" >car_engine</td>\n",
       "      <td id=\"T_d4ac1_row4_col1\" class=\"data row4 col1\" >Car engine capacity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d4ac1_row5_col0\" class=\"data row5 col0\" >car_transmission</td>\n",
       "      <td id=\"T_d4ac1_row5_col1\" class=\"data row5 col1\" >Car transmission type (auto/manual)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d4ac1_row6_col0\" class=\"data row6 col0\" >mileage</td>\n",
       "      <td id=\"T_d4ac1_row6_col1\" class=\"data row6 col1\" >Car mileage when it was sold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d4ac1_row7_col0\" class=\"data row7 col0\" >accident</td>\n",
       "      <td id=\"T_d4ac1_row7_col1\" class=\"data row7 col1\" >Boolean flag to indicate if a car has been through an accident (0 no accident, 1 accident)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d4ac1_row8_col0\" class=\"data row8 col0\" >flood</td>\n",
       "      <td id=\"T_d4ac1_row8_col1\" class=\"data row8 col1\" >Boolean flag to indicate if a car has been through a flood (0 no flood damage, 1 flood damage)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d4ac1_row9_col0\" class=\"data row9 col0\" >color</td>\n",
       "      <td id=\"T_d4ac1_row9_col1\" class=\"data row9 col1\" >Color of the car being sold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d4ac1_row10_col0\" class=\"data row10 col0\" >purchase_date</td>\n",
       "      <td id=\"T_d4ac1_row10_col1\" class=\"data row10 col1\" >Date when the car was purchased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d4ac1_row11_col0\" class=\"data row11 col0\" >sales_date</td>\n",
       "      <td id=\"T_d4ac1_row11_col1\" class=\"data row11 col1\" >Date when the car was sold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d4ac1_row12_col0\" class=\"data row12 col0\" >price</td>\n",
       "      <td id=\"T_d4ac1_row12_col1\" class=\"data row12 col1\" >Price at which the car was sold</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x206b2b45820>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create variables description table\n",
    "variables_df = pd.DataFrame({\n",
    "    'Variable': [\n",
    "        'car_brand',\n",
    "        'car_model',\n",
    "        'car_variant',\n",
    "        'car_year',\n",
    "        'car_engine',\n",
    "        'car_transmission',\n",
    "        'mileage',\n",
    "        'accident',\n",
    "        'flood',\n",
    "        'color',\n",
    "        'purchase_date',\n",
    "        'sales_date',\n",
    "        'price'\n",
    "    ],\n",
    "    'Description': [\n",
    "        'Hashed value for a car brand (Honda, BMW, Proton, etc.)',\n",
    "        'Hashed value for a car model (Myvi, Saga, i3, etc.)',\n",
    "        'Hashed value for a car variant. Car variants can be repeated with different car brands and models. For example, Perodua Myvi & Honda City can both have variant G',\n",
    "        'Car production year as 4 digits integer',\n",
    "        'Car engine capacity',\n",
    "        'Car transmission type (auto/manual)',\n",
    "        'Car mileage when it was sold',\n",
    "        'Boolean flag to indicate if a car has been through an accident (0 no accident, 1 accident)',\n",
    "        'Boolean flag to indicate if a car has been through a flood (0 no flood damage, 1 flood damage)',\n",
    "        'Color of the car being sold',\n",
    "        'Date when the car was purchased',\n",
    "        'Date when the car was sold',\n",
    "        'Price at which the car was sold'\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Display the table with styled header\n",
    "display(variables_df.style\n",
    "       .hide(axis='index')\n",
    "       .set_properties(**{\n",
    "           'text-align': 'left',\n",
    "           'white-space': 'pre-wrap'\n",
    "       })\n",
    "       .set_table_styles([\n",
    "           {'selector': 'th',\n",
    "            'props': [('text-align', 'center'),\n",
    "                     ('background-color', '#f2f2f2'),\n",
    "                     ('font-weight', 'bold')]\n",
    "           }\n",
    "       ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7024f8d2-f0cb-42aa-a845-723f0ecf7c0d",
   "metadata": {},
   "source": [
    "## 4. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b73b200-e1e8-4508-9af1-672f5dd92af8",
   "metadata": {},
   "source": [
    "### 4.1. Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57b585c-de64-4aa6-96e3-a9b9add3fad4",
   "metadata": {},
   "source": [
    "The analysis utilizes essential data manipulation libraries (pandas, numpy) and machine learning libraries from scikit-learn for model development and evaluation. Additional libraries including XGBoost for gradient boosting implementation and visualization libraries (matplotlib, seaborn) are incorporated to enhance model performance and result presentation. The datetime library is included for handling temporal data features in the car sales dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7824a79d-b86d-4b09-8e81-4769f24055fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5c3c59-8607-4c83-85b4-938731b5d035",
   "metadata": {},
   "source": [
    "### 4.2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaead015-6e4c-4ed0-ae9c-ffde37f359ff",
   "metadata": {},
   "source": [
    "The car price prediction project began with extensive data preprocessing of a dataset containing 95,428 records across 13 columns. Initial data quality assessment revealed minimal missing values (4 rows), which were removed to ensure data integrity, resulting in 95,424 clean records. The preprocessing phase included encoding categorical variables (car brand, model, variant, transmission, and color) using LabelEncoder and transforming date fields into numerical features for model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fd5e0c6-82cc-4f3e-835b-e397f91e37a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial data shape: (95428, 13)\n",
      "\n",
      "Missing values in initial data:\n",
      "car_brand           2\n",
      "car_model           4\n",
      "car_variant         2\n",
      "car_year            2\n",
      "car_engine          2\n",
      "car_transmission    2\n",
      "milage              2\n",
      "accident            2\n",
      "flood               2\n",
      "color               2\n",
      "purchase_date       2\n",
      "sales_date          2\n",
      "price               2\n",
      "dtype: int64\n",
      "\n",
      "Shape after removing NaN values: (95424, 13)\n"
     ]
    }
   ],
   "source": [
    "# Load data and initial processing\n",
    "df = pd.read_csv('DS Case Study_ Car Sales Data.csv')\n",
    "print(\"\\nInitial data shape:\", df.shape)\n",
    "print(\"\\nMissing values in initial data:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Remove rows with NaN values\n",
    "df = df.dropna()\n",
    "print(\"\\nShape after removing NaN values:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0b035f-42a1-4621-9fd6-7464fa0c1664",
   "metadata": {},
   "source": [
    "### 4.3. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59962b53-24fe-4598-af6d-7bdf4cdc76a4",
   "metadata": {},
   "source": [
    "Feature engineering played a crucial role in enhancing the model's predictive power. Several types of features were created: temporal features (days_until_sale, sales_month, sales_year, car_age) to capture time-based patterns; interaction features (age_milage) to represent combined effects; technical features (engine_transmission); and risk features (damage_score combining accident and flood indicators). The original datetime columns (purchase_date and sales_date) were dropped after extracting useful numerical features because machine learning models cannot directly process datetime objects, and all relevant temporal information had already been captured in the engineered features. This comprehensive feature engineering approach aimed to capture complex relationships within the data while ensuring all features were in a format suitable for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25cbd764-a49c-450a-aef5-ad7e83cba74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available features: ['car_brand', 'car_model', 'car_variant', 'car_year', 'car_engine', 'car_transmission', 'milage', 'accident', 'flood', 'color', 'price', 'days_until_sale', 'sales_month', 'sales_year', 'car_age']\n",
      "\n",
      "Final features: ['car_brand', 'car_model', 'car_variant', 'car_year', 'car_engine', 'car_transmission', 'milage', 'accident', 'flood', 'color', 'price', 'days_until_sale', 'sales_month', 'sales_year', 'car_age', 'age_milage', 'engine_transmission', 'damage_score']\n"
     ]
    }
   ],
   "source": [
    "# Convert dates\n",
    "df['purchase_date'] = pd.to_datetime(df['purchase_date'], format='%d/%m/%y')\n",
    "df['sales_date'] = pd.to_datetime(df['sales_date'], format='%d/%m/%y')\n",
    "\n",
    "# Extract temporal features\n",
    "df['days_until_sale'] = (df['sales_date'] - df['purchase_date']).dt.days\n",
    "df['sales_month'] = df['sales_date'].dt.month\n",
    "df['sales_year'] = df['sales_date'].dt.year\n",
    "df['car_age'] = df['sales_year'] - df['car_year']\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_cols = ['car_brand', 'car_model', 'car_variant', 'car_transmission', 'color']\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    label_encoders[col] = LabelEncoder()\n",
    "    df[col] = label_encoders[col].fit_transform(df[col])\n",
    "\n",
    "# Drop datetime columns\n",
    "df = df.drop(['purchase_date', 'sales_date'], axis=1)\n",
    "print(\"\\nAvailable features:\", df.columns.tolist())\n",
    "\n",
    "# Create interaction features\n",
    "df['age_milage'] = df['car_age'] * df['milage']\n",
    "df['engine_transmission'] = df['car_engine'] * df['car_transmission']\n",
    "df['damage_score'] = df['accident'] + df['flood']\n",
    "\n",
    "print(\"\\nFinal features:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de85b8ed-4e4d-4182-b1d5-ea3046317137",
   "metadata": {},
   "source": [
    "### 4.4. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d3f02a-5b80-4d2e-ae29-306fea936750",
   "metadata": {},
   "source": [
    "The feature importance analysis, conducted using Random Forest with a 1% importance threshold, revealed seven key predictors. The most influential features were car_year (30.97%) and car_engine (30.68%), which together account for over 60% of the predictive power. Brand-related features showed significant importance, with car_brand (19.35%) and car_model (7.40%) collectively contributing nearly 27%. The engineered feature age_milage (4.57%) proved valuable, demonstrating the effectiveness of our feature engineering approach. Car_age (2.05%) and car_variant (1.97%) were also identified as meaningful predictors. Based on the 1% threshold criterion, the model retained these seven features while excluding others with minimal impact, such as temporal features (sales_month, days_until_sale), condition indicators (accident, flood), and auxiliary characteristics (color, transmission). This streamlined feature set maintains model efficiency while preserving the most predictive variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0588e781-2e18-4499-8755-20c213fda07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Importance:\n",
      "                feature  importance\n",
      "3              car_year    0.309676\n",
      "4            car_engine    0.306830\n",
      "0             car_brand    0.193475\n",
      "1             car_model    0.073979\n",
      "14           age_milage    0.045717\n",
      "13              car_age    0.020505\n",
      "2           car_variant    0.019672\n",
      "11          sales_month    0.008741\n",
      "6                milage    0.008316\n",
      "10      days_until_sale    0.003847\n",
      "9                 color    0.003136\n",
      "12           sales_year    0.001824\n",
      "15  engine_transmission    0.001697\n",
      "5      car_transmission    0.001371\n",
      "7              accident    0.000641\n",
      "16         damage_score    0.000570\n",
      "8                 flood    0.000004\n",
      "\n",
      "Selected Features (importance > 1%):\n",
      "['car_year', 'car_engine', 'car_brand', 'car_model', 'age_milage', 'car_age', 'car_variant']\n"
     ]
    }
   ],
   "source": [
    "# Now proceed with feature importance analysis\n",
    "features = [col for col in df.columns if col != 'price']\n",
    "X = df[features]\n",
    "y = df['price']\n",
    "\n",
    "# Initial train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initial feature importance analysis\n",
    "initial_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "initial_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Analyze and display feature importance\n",
    "importances = initial_model.feature_importances_\n",
    "feature_imp = pd.DataFrame({\n",
    "    'feature': features,\n",
    "    'importance': importances\n",
    "})\n",
    "feature_imp = feature_imp.sort_values('importance', ascending=False)\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(feature_imp)\n",
    "\n",
    "# Feature selection based on importance threshold\n",
    "importance_threshold = 0.01  # 1% importance threshold\n",
    "selected_features = feature_imp[feature_imp['importance'] > importance_threshold]['feature'].tolist()\n",
    "print(\"\\nSelected Features (importance > 1%):\")\n",
    "print(selected_features)\n",
    "\n",
    "# Create new feature matrices with selected features only\n",
    "X_selected = X[selected_features]\n",
    "X_train_selected, X_test_selected = train_test_split(X_selected, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale selected features\n",
    "scaler_selected = StandardScaler()\n",
    "X_train_scaled = scaler_selected.fit_transform(X_train_selected)\n",
    "X_test_scaled = scaler_selected.transform(X_test_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308276dd-513f-4aad-abc6-98da4cabee8d",
   "metadata": {},
   "source": [
    "### 4.5. Model Development & Performance Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93ac68d-7ca9-4507-859c-1cae45fdf59e",
   "metadata": {},
   "source": [
    "Three different regression models were implemented and compared using the selected features from our feature importance analysis. After comparing the models' performance on the test set (unseen data), the Random Forest model emerged as the best performer with RMSE of 4358.27, MAE of 2426.90, and R-squared value of 0.98. XGBoost showed comparable performance with slightly higher error metrics (RMSE: 4513.91, MAE: 2721.57) and the same R² of 0.98, while Gradient Boosting demonstrated notably lower performance (RMSE: 9728.04, MAE: 5728.41, R²: 0.89).\n",
    "\n",
    "The selection of these evaluation metrics was deliberate: RMSE to penalize large prediction errors more heavily (making it particularly relevant for price predictions), MAE to provide easily interpretable error measurements in actual price units, and R-squared to indicate the model's overall explanatory power. The strong performance of both Random Forest and XGBoost, particularly their high R² values, suggests that either could be suitable for practical application, though Random Forest's lower error metrics make it the preferred choice for this specific car price prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f88de969-2872-44c5-802f-dbd3fcda678c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Results with Selected Features:\n",
      "\n",
      "RandomForest:\n",
      "RMSE: 4358.27\n",
      "MAE: 2426.90\n",
      "R2: 0.98\n",
      "\n",
      "XGBoost:\n",
      "RMSE: 4513.91\n",
      "MAE: 2721.57\n",
      "R2: 0.98\n",
      "\n",
      "GradientBoosting:\n",
      "RMSE: 9728.04\n",
      "MAE: 5728.41\n",
      "R2: 0.89\n",
      "\n",
      "Best Performing Model: RandomForest\n"
     ]
    }
   ],
   "source": [
    "# Train models with selected features\n",
    "models = {\n",
    "    'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'XGBoost': xgb.XGBRegressor(random_state=42),\n",
    "    'GradientBoosting': GradientBoostingRegressor(random_state=42)\n",
    "}\n",
    "results = {}\n",
    "best_model = None\n",
    "best_score = float('inf')\n",
    "best_model_name = None\n",
    "\n",
    "# Train and evaluate each model with selected features\n",
    "for name, model in models.items():\n",
    "    # Train model\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predictions on test set\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2': r2\n",
    "    }\n",
    "    \n",
    "    # Update best model if current is better\n",
    "    if rmse < best_score:\n",
    "        best_score = rmse\n",
    "        best_model = model\n",
    "        best_model_name = name\n",
    "\n",
    "# Compare results before and after feature selection\n",
    "print(\"\\nModel Results with Selected Features:\")\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    for metric_name, value in metrics.items():\n",
    "        print(f\"{metric_name}: {value:.2f}\")\n",
    "\n",
    "# Print best model information\n",
    "print(f\"\\nBest Performing Model: {best_model_name}\")\n",
    "\n",
    "# Generate predictions using best model\n",
    "X_all_selected = X[selected_features]\n",
    "X_all_scaled = scaler_selected.transform(X_all_selected)  # Use scaler_selected instead of scaler\n",
    "predictions = best_model.predict(X_all_scaled)\n",
    "\n",
    "# Create output DataFrame with predictions\n",
    "output_df = df.copy()\n",
    "output_df['predicted_price'] = predictions\n",
    "output_df['price_difference'] = output_df['price'] - output_df['predicted_price']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32931cd-1e3b-4886-9898-aaf3e8d49019",
   "metadata": {},
   "source": [
    "### 4.6. Result & Export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ce77c4-0ec3-4600-affc-8e29de84737c",
   "metadata": {},
   "source": [
    "The model's predictions were saved in 'car_price_predictions.csv', which includes the original features along with three additional columns:\n",
    "\n",
    "- Price: The actual selling price of the car (e.g., 30100.0, 32100.0)\n",
    "\n",
    "- Predicted Price: The model's price prediction (e.g., 32282.0, 32187.0)\n",
    "\n",
    "- Price Difference: The difference between actual and predicted prices (Actual - Predicted), helping identify prediction accuracy. A negative value indicates over-prediction (e.g., -2182.0), while a positive value indicates under-prediction (e.g., 425.0).\n",
    "\n",
    "Sample predictions from the dataset show that the model's predictions were generally close to actual prices, with differences ranging from relatively small (-87.0) to more substantial (-4369.0). Most predictions stayed within a reasonable range, demonstrating the model's ability to capture market values effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "943b4133-774c-4786-974c-a4564125faf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions saved to 'car_price_predictions.csv'\n",
      "\n",
      "Sample of Predictions:\n",
      "   car_brand  car_model  car_year    milage    price  predicted_price  \\\n",
      "0         22         52    2010.0  282508.0  30100.0          32282.0   \n",
      "1         22         52    2010.0  169475.0  32100.0          32187.0   \n",
      "2         22         52    2010.0  105276.0  34100.0          33675.0   \n",
      "3         22         52    2010.0   81123.0  40100.0          40501.0   \n",
      "4         22         52    2013.0  157239.0  42000.0          46309.0   \n",
      "5         22         52    2013.0  170215.0  50500.0          50975.0   \n",
      "6         22         52    2013.0  192647.0  55100.0          54908.0   \n",
      "7         22         52    2013.0   81127.0  57400.0          58306.0   \n",
      "8         22         52    2013.0  171512.0  58800.0          55269.0   \n",
      "9         22         52    2013.0   98656.0  60300.0          60644.0   \n",
      "\n",
      "   price_difference  \n",
      "0           -2182.0  \n",
      "1             -87.0  \n",
      "2             425.0  \n",
      "3            -401.0  \n",
      "4           -4309.0  \n",
      "5            -475.0  \n",
      "6             192.0  \n",
      "7            -906.0  \n",
      "8            3531.0  \n",
      "9            -344.0  \n"
     ]
    }
   ],
   "source": [
    "# Save and display results\n",
    "output_df.to_csv('car_price_predictions2.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"\\nPredictions saved to 'car_price_predictions.csv'\")\n",
    "\n",
    "# Display sample of predictions\n",
    "print(\"\\nSample of Predictions:\")\n",
    "sample_cols = ['car_brand', 'car_model', 'car_year', 'milage', 'price', 'predicted_price', 'price_difference']\n",
    "print(output_df[sample_cols].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77ce30b-a44d-42ce-b0cc-ee420cb57a16",
   "metadata": {},
   "source": [
    "## 5. Discussion and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac6bb90-2abe-4093-aa9a-d2971d903aba",
   "metadata": {},
   "source": [
    "### 5.1. Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b363dabb-72e0-4f01-b515-7c83129a60d6",
   "metadata": {},
   "source": [
    "The car price prediction model reveals important patterns in the relationship between actual and predicted prices, providing valuable insights for various stakeholders in the used car market. When the model overpredicts (negative price difference), such as predicting 32282.0 for a car actually priced at 30100.0 (difference of -2182.0), it suggests the vehicle might be selling below its expected market value - potentially indicating good buying opportunities or cases requiring investigation into why the price is lower than market expectations. \n",
    "\n",
    "Conversely, underprediction (positive price difference), as seen in the case where a car priced at 58800.0 was predicted at 55269.0 (difference of 3531.0), indicates vehicles selling above their expected market value, which could signal premium features not captured by our model or potential overpricing situations. \n",
    "\n",
    "These prediction patterns, combined with our feature importance findings where basic car specifications (year, engine, brand) heavily outweigh condition factors (accident, flood damage), suggest a market where fundamental vehicle characteristics primarily drive pricing decisions. This insight makes our model particularly valuable for dealers in pricing strategy optimization, buyers in deal identification, sellers in competitive price setting, and market analysts in understanding price anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcc4824-84fc-49d9-8de4-f1d20dd44d07",
   "metadata": {},
   "source": [
    "### 5.2. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddb31d0-2c40-4729-99c3-29b2afbf4974",
   "metadata": {},
   "source": [
    "The study developed a robust car price prediction model achieving high accuracy (R² = 0.98) on the test set, with Random Forest emerging as the best performer (RMSE: 4358.27, MAE: 2426.90). Feature engineering and selection identified seven key features that effectively capture price determinants, with car specifications (year, engine, brand) being most influential. The model's ability to identify over and under-priced vehicles makes it a valuable tool for market participants in making informed pricing decisions. Future work could explore incorporating additional features like market trends or regional factors to further enhance prediction accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddb1fa0-d059-4161-bf72-35003846ef95",
   "metadata": {},
   "source": [
    "## 6. Appendix: Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc2c109-a475-4c2f-bce3-37c0b29e807c",
   "metadata": {},
   "source": [
    "For replication and future reference, the complete code implementation has been consolidated into a single main function in the appendix section. The function encompasses all steps from data preprocessing through model evaluation and prediction generation, making it easier to reproduce the analysis pipeline with different datasets or parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "04e596a2-98ac-4085-9501-e54c128f6455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial data shape: (95428, 13)\n",
      "\n",
      "Missing values in initial data:\n",
      "car_brand           2\n",
      "car_model           4\n",
      "car_variant         2\n",
      "car_year            2\n",
      "car_engine          2\n",
      "car_transmission    2\n",
      "milage              2\n",
      "accident            2\n",
      "flood               2\n",
      "color               2\n",
      "purchase_date       2\n",
      "sales_date          2\n",
      "price               2\n",
      "dtype: int64\n",
      "\n",
      "Shape after removing NaN values: (95424, 13)\n",
      "Available columns: ['car_brand', 'car_model', 'car_variant', 'car_year', 'car_engine', 'car_transmission', 'milage', 'accident', 'flood', 'color', 'purchase_date', 'sales_date', 'price']\n",
      "\n",
      "Final features: ['car_brand', 'car_model', 'car_variant', 'car_year', 'car_engine', 'car_transmission', 'milage', 'accident', 'flood', 'color', 'price', 'days_until_sale', 'sales_month', 'sales_year', 'car_age', 'age_milage', 'engine_transmission', 'damage_score']\n",
      "\n",
      "Performing feature selection...\n",
      "\n",
      "Feature Importance Before Selection:\n",
      "                feature  importance\n",
      "3              car_year    0.309676\n",
      "4            car_engine    0.306830\n",
      "0             car_brand    0.193475\n",
      "1             car_model    0.073979\n",
      "14           age_milage    0.045717\n",
      "13              car_age    0.020505\n",
      "2           car_variant    0.019672\n",
      "11          sales_month    0.008741\n",
      "6                milage    0.008316\n",
      "10      days_until_sale    0.003847\n",
      "9                 color    0.003136\n",
      "12           sales_year    0.001824\n",
      "15  engine_transmission    0.001697\n",
      "5      car_transmission    0.001371\n",
      "7              accident    0.000641\n",
      "16         damage_score    0.000570\n",
      "8                 flood    0.000004\n",
      "\n",
      "Selected Features (importance > 1%):\n",
      "['car_year', 'car_engine', 'car_brand', 'car_model', 'age_milage', 'car_age', 'car_variant']\n",
      "\n",
      "Model Results (with selected features):\n",
      "\n",
      "RandomForest:\n",
      "RMSE: 4358.27\n",
      "MAE: 2426.90\n",
      "R2: 0.98\n",
      "\n",
      "XGBoost:\n",
      "RMSE: 4513.91\n",
      "MAE: 2721.57\n",
      "R2: 0.98\n",
      "\n",
      "GradientBoosting:\n",
      "RMSE: 9728.04\n",
      "MAE: 5728.41\n",
      "R2: 0.89\n",
      "\n",
      "Best Performing Model: RandomForest\n",
      "\n",
      "Predictions saved to 'car_price_predictions.csv'\n",
      "\n",
      "Sample of Predictions:\n",
      "                                           car_brand  \\\n",
      "0  b51026e4444f98ecdbe1d7cb1f310427a47d7a6e7659b3...   \n",
      "1  b51026e4444f98ecdbe1d7cb1f310427a47d7a6e7659b3...   \n",
      "2  b51026e4444f98ecdbe1d7cb1f310427a47d7a6e7659b3...   \n",
      "3  b51026e4444f98ecdbe1d7cb1f310427a47d7a6e7659b3...   \n",
      "4  b51026e4444f98ecdbe1d7cb1f310427a47d7a6e7659b3...   \n",
      "5  b51026e4444f98ecdbe1d7cb1f310427a47d7a6e7659b3...   \n",
      "6  b51026e4444f98ecdbe1d7cb1f310427a47d7a6e7659b3...   \n",
      "7  b51026e4444f98ecdbe1d7cb1f310427a47d7a6e7659b3...   \n",
      "8  b51026e4444f98ecdbe1d7cb1f310427a47d7a6e7659b3...   \n",
      "9  b51026e4444f98ecdbe1d7cb1f310427a47d7a6e7659b3...   \n",
      "\n",
      "                                           car_model  car_year    milage  \\\n",
      "0  4539e4b4889079c2a00afeae0bfc1439840ef2379a1fb8...    2010.0  282508.0   \n",
      "1  4539e4b4889079c2a00afeae0bfc1439840ef2379a1fb8...    2010.0  169475.0   \n",
      "2  4539e4b4889079c2a00afeae0bfc1439840ef2379a1fb8...    2010.0  105276.0   \n",
      "3  4539e4b4889079c2a00afeae0bfc1439840ef2379a1fb8...    2010.0   81123.0   \n",
      "4  4539e4b4889079c2a00afeae0bfc1439840ef2379a1fb8...    2013.0  157239.0   \n",
      "5  4539e4b4889079c2a00afeae0bfc1439840ef2379a1fb8...    2013.0  170215.0   \n",
      "6  4539e4b4889079c2a00afeae0bfc1439840ef2379a1fb8...    2013.0  192647.0   \n",
      "7  4539e4b4889079c2a00afeae0bfc1439840ef2379a1fb8...    2013.0   81127.0   \n",
      "8  4539e4b4889079c2a00afeae0bfc1439840ef2379a1fb8...    2013.0  171512.0   \n",
      "9  4539e4b4889079c2a00afeae0bfc1439840ef2379a1fb8...    2013.0   98656.0   \n",
      "\n",
      "     price  predicted_price  price_difference  \n",
      "0  30100.0          32282.0           -2182.0  \n",
      "1  32100.0          32187.0             -87.0  \n",
      "2  34100.0          33675.0             425.0  \n",
      "3  40100.0          40501.0            -401.0  \n",
      "4  42000.0          46309.0           -4309.0  \n",
      "5  50500.0          50975.0            -475.0  \n",
      "6  55100.0          54908.0             192.0  \n",
      "7  57400.0          58306.0            -906.0  \n",
      "8  58800.0          55269.0            3531.0  \n",
      "9  60300.0          60644.0            -344.0  \n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the data\n",
    "def preprocess_data(df):\n",
    "    print(\"Available columns:\", df.columns.tolist())\n",
    "    \n",
    "    # Convert dates to datetime with correct format 'DD/MM/YY'\n",
    "    df['purchase_date'] = pd.to_datetime(df['purchase_date'], format='%d/%m/%y')\n",
    "    df['sales_date'] = pd.to_datetime(df['sales_date'], format='%d/%m/%y')\n",
    "    \n",
    "    # Extract numeric features from dates\n",
    "    df['days_until_sale'] = (df['sales_date'] - df['purchase_date']).dt.days\n",
    "    df['sales_month'] = df['sales_date'].dt.month\n",
    "    df['sales_year'] = df['sales_date'].dt.year\n",
    "    \n",
    "    # Calculate car age at time of sale\n",
    "    df['car_age'] = df['sales_year'] - df['car_year']\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    categorical_cols = ['car_brand', 'car_model', 'car_variant', 'car_transmission', 'color']\n",
    "    label_encoders = {}\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        label_encoders[col] = LabelEncoder()\n",
    "        df[col] = label_encoders[col].fit_transform(df[col])\n",
    "    \n",
    "    # Drop the original datetime columns as they can't be used in the model\n",
    "    df = df.drop(['purchase_date', 'sales_date'], axis=1)\n",
    "    \n",
    "    return df, label_encoders\n",
    "\n",
    "# Feature engineering\n",
    "def create_features(df):\n",
    "    # Create interaction features\n",
    "    df['age_milage'] = df['car_age'] * df['milage']\n",
    "    df['engine_transmission'] = df['car_engine'] * df['car_transmission']\n",
    "    \n",
    "    # Create damage score\n",
    "    df['damage_score'] = df['accident'] + df['flood']\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Feature importance and selection analysis\n",
    "def analyze_and_select_features(X_train_scaled, y_train, features, threshold=0.01):\n",
    "    # Initial model for feature importance\n",
    "    initial_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    initial_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Get feature importance\n",
    "    importances = initial_model.feature_importances_\n",
    "    feature_imp = pd.DataFrame({\n",
    "        'feature': features,\n",
    "        'importance': importances\n",
    "    })\n",
    "    feature_imp = feature_imp.sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Select features above threshold\n",
    "    selected_features = feature_imp[feature_imp['importance'] > threshold]['feature'].tolist()\n",
    "    \n",
    "    return feature_imp, selected_features\n",
    "\n",
    "# Model training and evaluation\n",
    "def train_evaluate_model(X_train, X_test, y_train, y_test):\n",
    "    models = {\n",
    "        'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "        'XGBoost': xgb.XGBRegressor(random_state=42),\n",
    "        'GradientBoosting': GradientBoostingRegressor(random_state=42)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    best_model = None\n",
    "    best_score = float('inf')\n",
    "    best_model_name = None  # Add variable for storing best model name\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        # Train model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        results[name] = {\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae,\n",
    "            'R2': r2\n",
    "        }\n",
    "        \n",
    "        # Update best model if current is better\n",
    "        if rmse < best_score:\n",
    "            best_score = rmse\n",
    "            best_model = model\n",
    "            best_model_name = name\n",
    "    \n",
    "    return results, best_model\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    # Load data\n",
    "    df = pd.read_csv('DS Case Study_ Car Sales Data.csv')\n",
    "    print(\"\\nInitial data shape:\", df.shape)\n",
    "    print(\"\\nMissing values in initial data:\")\n",
    "    print(df.isnull().sum())\n",
    "    \n",
    "    # Remove rows with NaN values\n",
    "    df = df.dropna()\n",
    "    print(\"\\nShape after removing NaN values:\", df.shape)\n",
    "    \n",
    "    # Store original data before preprocessing\n",
    "    original_data = df.copy()\n",
    "    \n",
    "    # Preprocess data\n",
    "    df_processed, encoders = preprocess_data(df)\n",
    "    df_processed = create_features(df_processed)\n",
    "    \n",
    "    # Print feature names\n",
    "    print(\"\\nFinal features:\", df_processed.columns.tolist())\n",
    "    \n",
    "    # Prepare features and target\n",
    "    features = [col for col in df_processed.columns if col != 'price']\n",
    "    X = df_processed[features]\n",
    "    y = df_processed['price']\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Perform feature selection\n",
    "    print(\"\\nPerforming feature selection...\")\n",
    "    feature_imp, selected_features = analyze_and_select_features(X_train_scaled, y_train, features)\n",
    "    print(\"\\nFeature Importance Before Selection:\")\n",
    "    print(feature_imp)\n",
    "    print(f\"\\nSelected Features (importance > 1%):\")\n",
    "    print(selected_features)\n",
    "    \n",
    "    # Create new feature matrices with selected features\n",
    "    X_train_selected = X_train_scaled[:, [features.index(feat) for feat in selected_features]]\n",
    "    X_test_selected = X_test_scaled[:, [features.index(feat) for feat in selected_features]]\n",
    "    \n",
    "    # Train and evaluate models with selected features\n",
    "    results, best_model = train_evaluate_model(X_train_selected, X_test_selected, y_train, y_test)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nModel Results (with selected features):\")\n",
    "    for model_name, metrics in results.items():\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        for metric_name, value in metrics.items():\n",
    "            print(f\"{metric_name}: {value:.2f}\")\n",
    "   \n",
    "    # Print best model information\n",
    "    print(f\"\\nBest Performing Model: {best_model_name}\")\n",
    "\n",
    "    # Generate predictions for all data using selected features\n",
    "    X_all_scaled = scaler.transform(X)\n",
    "    X_all_selected = X_all_scaled[:, [features.index(feat) for feat in selected_features]]\n",
    "    predictions = best_model.predict(X_all_selected)\n",
    "    \n",
    "    # Create output DataFrame\n",
    "    output_df = original_data.copy()\n",
    "    output_df['predicted_price'] = predictions\n",
    "    output_df['price_difference'] = output_df['price'] - output_df['predicted_price']\n",
    "    \n",
    "    # Save predictions to CSV\n",
    "    output_df.to_csv('car_price_predictions.csv', index=False, encoding='utf-8-sig')\n",
    "    print(\"\\nPredictions saved to 'car_price_predictions.csv'\")\n",
    "    \n",
    "    # Display sample of predictions\n",
    "    print(\"\\nSample of Predictions:\")\n",
    "    sample_cols = ['car_brand', 'car_model', 'car_year', 'milage', 'price', 'predicted_price', 'price_difference']\n",
    "    print(output_df[sample_cols].head(10))\n",
    "    \n",
    "    return results, best_model, feature_imp, output_df\n",
    "\n",
    "# Run analysis\n",
    "if __name__ == \"__main__\":\n",
    "    results, best_model, feature_importance, output_df = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
